# Adversarial Framework for Real-Time Attacks

## Описание проекта

Фреймворк предназначен для проведения исследований в области уязвимостей моделей машинного обучения, функционирующих в режиме реального времени. Основной акцент сделан на генерации атакующих воздействий (adversarial perturbations) с применением методов обучения с подкреплением. Проект ориентирован на тестирование устойчивости моделей компьютерного зрения, обрабатывающих видеопотоки, в том числе в условиях ограниченной наблюдаемости и адаптивной среды.

Фреймворк реализует архитектуру, позволяющую:

- моделировать онлайн-атаки;
- использовать различные алгоритмы генерации возмущений;
- логировать, визуализировать и анализировать эффективность атакующих стратегий.

---

## Основные возможности

- Проведение adversarial-атак на потоковые модели машинного обучения.
- Использование алгоритмов PPO (Proximal Policy Optimization) для адаптивных атак.
- Поддержка классических атак: RL, FGSM, PGD, CW.
- Визуализация атак в реальном времени с отображением исходных и модифицированных изображений.
- Поддержка структурированного логирования в формате JSON.
- Возможность расширения за счёт модульной архитектуры.

---

## Используемые технологии

- **Язык программирования:** Python 3.10+
- **Фреймворки и библиотеки:**  
  - PyTorch  
  - OpenCV  
  - Stable-Baselines3  
  - Numpy, Matplotlib  
  - YAML

---

## Архитектура проекта
adversarial_framework/
├── .gitignore                 # Исключения для Git
├── core/                      # Базовая логика фреймворка: модели, окружение, атаки, политики
│   ├── __init__.py             # Инициализация пакета
│   ├── factory.py              # Фабрика создания моделей и сред
│   ├── model.py                # Свёрточная модель классификации
│   ├── custom_policy.py        # Специализированные политики PPO
│   ├── utils.py                # Вспомогательные функции
│   ├── custom_cnn.py           # Кастомная архитектура сверточной сети
│   ├── callbacks.py            # Колбэки для обучения агентов
│   ├── environment.py          # Эмуляция среды обработки видеопотока
│   ├── attacks.py              # Реализация атак RL, FGSM, PGD и других
│   └── __pycache__/            # Скомпилированные Python-файлы
├── agents/                     # Предобученные агенты RL
│   └── rl_agent.zip             # Архив с агентом PPO
├── logs/                       # Журналы атак и обучения
├── configs/                    # Конфигурационные YAML-файлы и зависимости
│   ├── attack_config.yaml       # Параметры атак
│   ├── train_config.yaml        # Параметры обучения
│   └── requirements.txt         # Зависимости проекта
├── scripts/                    # Скрипты запуска и визуализации
│   ├── framework.py             # Запуск атак на поток данных
│   ├── train_agent.py           # Обучение PPO-агента
│   ├── visualize_logs.py        # Визуализация логов атак
│   └── all_json.py              # Обработка журналов логов


---

## Установка

1. Клонируйте репозиторий:
```
git clone https://github.com/yourusername/adversarial_framework.git
cd adversarial_framework
```
2. Установите зависимости:
```
   pip install -r configs/requirements.txt
```  
Запуск

▶ Проведение атаки в реальном времени
```
python scripts/framework.py
```
Этот скрипт запускает камеру, применяет атакующие стратегии и отображает результаты.

▶ Обучение RL-агента
```
python scripts/train_agent.py
```
Файл использует конфигурации из train_config.yaml и сохраняет агента в agents/.

▶ Визуализация логов
```
python scripts/visualize_logs.py
```
Позволяет проанализировать сохранённые результаты атак (JSON) в виде графиков и визуальных сравнений.

▶ Объединение всех логов в один файл для глобального анализа.
```
python scripts/all_json.py
```
Этот скрипт агрегирует данные из нескольких журналов (JSON-файлов) в единый сводный отчёт для дальнейшего анализа и визуализации. Он автоматически объединяет результаты нескольких запусков атак в одну структуру данных.

Конфигурация проекта: подробное описание параметров

Файл attack_config.yaml
Настройки для проведения атак в реальном времени.

Параметр	                     Описание
attack.type	                   Тип атаки: 'rl_attack' (атака с использованием агента RL).
attack.params.epsilon	         Максимальная величина допустимого возмущения, добавляемого к изображениям. Большие значения делают атаку более заметной.
attack.params.alpha	           Размер шага изменения на каждой итерации атаки. Используется в методах типа PGD.
attack.params.num_steps	       Количество шагов/итераций, в течение которых происходит атака.
attack.params.confidence	     Целевое значение уверенности модели после атаки. Чем выше значение, тем "агрессивнее" атака.
attack.params.lr	             Скорость обучения агента обучения с подкреплением. Влияет на скорость адаптации стратегии атаки.
attack.params.env_name	       Идентификатор среды для обучения агента. Обычно название кастомного окружения Gym.
attack.params.agent_path	     Путь к сохранённой модели агента PPO, использующегося для генерации атак.
attack.params.device	         Устройство для вычислений: 'cpu' (центральный процессор) или 'cuda' (графический ускоритель).
attack.params.visualize	       Флаг (True/False), разрешающий/запрещающий визуализацию атак в реальном времени.
attack.params.save_results	   Флаг (True/False), определяющий необходимость сохранения результатов атак в лог-файлы.

Параметр	                     Описание
model.type	                   Тип используемой модели: 'pytorch' (PyTorch-модель).
model.params.input_size	       Размерность входных данных в формате [каналы, высота, ширина], например [3, 112, 112].
model.params.num_classes	     Количество целевых классов классификации, которые должна различать модель.

Параметр	                     Описание
data.source	                   Источник данных для атак: 'webcam' — видеопоток с веб-камеры.
data.preprocess.resize	       Размер к которому масштабируется каждое изображение перед подачей в модель.
data.preprocess.normalize	     Флаг (True/False), нормализовать ли значения пикселей изображения для корректной работы модели.


Файл train_config.yaml
Настройки для обучения агента PPO.

Параметр	                                                          Описание
hyperparams.policy_type	                                            Тип архитектуры политики агента: 'CnnPolicy' (обычная свёрточная сеть) или 'CustomPolicy' (пользовательская модификация).
hyperparams.total_timesteps	                                        Общее количество взаимодействий агента со средой (чем больше, тем качественнее обучение).
hyperparams.n_envs	                                                Количество параллельных сред для обучения (ускоряет процесс обучения за счёт многопоточности).
hyperparams.learning_rate	                                          Скорость обучения (обычно очень маленькое значение для стабильного процесса обучения).
hyperparams.policy_kwargs.features_extractor_class                  Класс для извлечения признаков из изображений (например, 'CustomCNN').
hyperparams.policy_kwargs.features_extractor_kwargs.features_dim	  Размерность вектора признаков, извлечённого из изображения.
hyperparams.max_grad_norm	                                          Ограничение нормы градиента для стабилизации обновлений нейросети.
hyperparams.clip_range	                                            Диапазон обрезки изменения стратегии агента в алгоритме PPO.
hyperparams.ent_coef	                                              Коэффициент энтропийного регуляризатора, который стимулирует исследование новых стратегий.

Параметр	                                                          Описание
paths.agents	                                                      Путь для сохранения обученных моделей агентов.
paths.logs	                                                        Путь для сохранения логов обучения агента и хода атак.


Пример применения
Фреймворк был использован для атаки на модель классификации, обрабатывающую изображения из видеопотока. Результаты включают:

Средний уровень успешности атак: 76%

Средняя L2-дистанция между оригиналом и возмущением: 18.1

Средняя разница уверенности модели: 0.0141

Атаки оказывались малозаметными для человека, но эффективными против модели.

Ограничения
Требуется доступ к видеопотоку (например, веб-камера)

RL-агенты чувствительны к конфигурации среды

Для корректной работы с CUDA необходимо наличие GPU

Перспективы развития
Поддержка дополнительных видов атак (например, backdoor-атаки)

Интеграция с другими типами моделей (например, NLP)

Поддержка имитационного обучения (imitation learning)
