# Adversarial Framework for Real-Time Attacks

## Описание проекта

Фреймворк предназначен для проведения исследований в области уязвимостей моделей машинного обучения, функционирующих в режиме реального времени. Основной акцент сделан на генерации атакующих воздействий (*adversarial perturbations*) с применением методов обучения с подкреплением. Проект ориентирован на тестирование устойчивости моделей компьютерного зрения, обрабатывающих видеопотоки, включая условия ограниченной наблюдаемости и адаптивной среды.

Фреймворк реализует архитектуру, позволяющую:

- моделировать онлайн-атаки;
- использовать различные алгоритмы генерации возмущений;
- логировать, визуализировать и анализировать эффективность атакующих стратегий.

---

## Основные возможности

- Проведение adversarial-атак на потоковые модели машинного обучения.
- Использование алгоритма PPO (Proximal Policy Optimization) для адаптивных атак.
- Поддержка классических атак: RL, FGSM, PGD, CW.
- Визуализация атак в реальном времени с отображением исходных и модифицированных изображений.
- Поддержка структурированного логирования в формате JSON.
- Возможность расширения за счёт модульной архитектуры.

---

## Используемые технологии

- **Python 3.10+**
- **Фреймворки и библиотеки:**
  - PyTorch
  - OpenCV
  - Stable-Baselines3
  - Numpy, Matplotlib
  - YAML

---

## Архитектура проекта

```
adversarial_framework/
├── .gitignore                 # Исключения для Git
├── core/                      # Базовая логика фреймворка: модели, окружение, атаки, политики
│   ├── __init__.py             # Инициализация пакета
│   ├── factory.py              # Фабрика создания моделей и сред
│   ├── model.py                # Свёрточная модель классификации
│   ├── custom_policy.py        # Специализированные политики PPO
│   ├── utils.py                # Вспомогательные функции
│   ├── custom_cnn.py           # Кастомная архитектура свёрточной сети
│   ├── callbacks.py            # Колбэки для обучения агентов
│   ├── environment.py          # Эмуляция среды обработки видеопотока
│   ├── attacks.py              # Реализация атак RL, FGSM, PGD и других
│   └── __pycache__/            # Скомпилированные Python-файлы
├── agents/                     # Предобученные агенты RL
│   └── rl_agent.zip             # Архив с агентом PPO
├── logs/                       # Журналы атак и обучения
├── configs/                    # Конфигурационные YAML-файлы и зависимости
│   ├── attack_config.yaml       # Параметры атак
│   ├── train_config.yaml        # Параметры обучения
│   └── requirements.txt         # Зависимости проекта
├── scripts/                    # Скрипты запуска и визуализации
│   ├── framework.py             # Запуск атак на поток данных
│   ├── train_agent.py           # Обучение PPO-агента
│   ├── visualize_logs.py        # Визуализация логов атак
│   └── all_json.py              # Объединение логов атак
```

---

## Установка

1. Клонируйте репозиторий:
    ```bash
    git clone https://github.com/yourusername/adversarial_framework.git
    cd adversarial_framework
    ```
2. Установите зависимости:
    ```bash
    pip install -r configs/requirements.txt
    ```

---

## Запуск

▶ **Проведение атаки в реальном времени:**
```bash
python scripts/framework.py
```
Запускает видеопоток, применяет атакующие стратегии и отображает результаты в реальном времени.

▶ **Обучение RL-агента:**
```bash
python scripts/train_agent.py
```
Файл использует конфигурацию из `train_config.yaml` и сохраняет обученного агента в папку `agents/`.

▶ **Визуализация логов:**
```bash
python scripts/visualize_logs.py
```
Позволяет построить графики успешности атак, уверенности модели и визуализировать сравнения.

▶ **Объединение всех логов:**
```bash
python scripts/all_json.py
```
Агрегирует результаты из множества журналов JSON в один файл для глобального анализа.

---

## Конфигурация проекта

### Файл `attack_config.yaml`

| Параметр | Описание |
|:---|:---|
| `attack.type` | Тип атаки (`rl_attack`). |
| `attack.params.epsilon` | Максимальная величина допустимого возмущения. |
| `attack.params.alpha` | Размер шага изменения на каждой итерации атаки. |
| `attack.params.num_steps` | Количество итераций атаки. |
| `attack.params.confidence` | Целевая уверенность модели после атаки. |
| `attack.params.lr` | Скорость обучения агента RL. |
| `attack.params.env_name` | Название среды обучения. |
| `attack.params.agent_path` | Путь к предобученному агенту PPO. |
| `attack.params.device` | Вычислительное устройство (`cpu` или `cuda`). |
| `attack.params.visualize` | Включение визуализации атак. |
| `attack.params.save_results` | Сохранение результатов атак. |
| `model.type` | Тип модели (`pytorch`). |
| `model.params.input_size` | Размерность входных данных `[каналы, высота, ширина]`. |
| `model.params.num_classes` | Количество классов классификации. |
| `data.source` | Источник данных (`webcam`). |
| `data.preprocess.resize` | Размер изменения изображений перед подачей в модель. |
| `data.preprocess.normalize` | Нормализация значений пикселей. |

---

### Файл `train_config.yaml`

| Параметр | Описание |
|:---|:---|
| `hyperparams.policy_type` | Тип политики агента (`CnnPolicy` или `CustomPolicy`). |
| `hyperparams.total_timesteps` | Общее количество шагов обучения. |
| `hyperparams.n_envs` | Количество параллельных окружений. |
| `hyperparams.learning_rate` | Скорость обучения агента. |
| `hyperparams.policy_kwargs.features_extractor_class` | Класс извлечения признаков (`CustomCNN`). |
| `hyperparams.policy_kwargs.features_extractor_kwargs.features_dim` | Размерность выходных признаков. |
| `hyperparams.max_grad_norm` | Ограничение нормы градиента. |
| `hyperparams.clip_range` | Диапазон обрезки стратегии PPO. |
| `hyperparams.ent_coef` | Коэффициент энтропийного регуляризатора. |
| `paths.agents` | Путь для сохранения обученных агентов. |
| `paths.logs` | Путь для сохранения логов. |

---

## Пример применения

Фреймворк был использован для атаки на модель классификации, обрабатывающую изображения из видеопотока. Получены следующие результаты:

- **Средний уровень успешности атак:** 76%
- **Средняя L2-дистанция между оригиналом и возмущением:** 18.1
- **Средняя разница уверенности модели:** 0.0141

Атаки были малозаметными для человеческого восприятия, но существенно снижали эффективность модели.

---

## Ограничения

- Требуется доступ к видеопотоку (например, веб-камера).
- Агенты чувствительны к конфигурации среды обучения.
- Для корректной работы с `cuda` необходима поддержка GPU.

---

## Перспективы развития

- Поддержка дополнительных видов атак (например, backdoor-атаки).
- Интеграция с другими типами моделей (например, NLP).
- Реализация имитационного обучения (imitation learning).
